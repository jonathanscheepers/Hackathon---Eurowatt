{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the required packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import words\n",
    "import nltk.sentiment.sentiment_analyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from spellchecker import SpellChecker\n",
    "import csv\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading th CSV that was exported with the tweets; the filename depends on the number of tweets\n",
    "train_tweets = pd.read_csv(r'C:\\Users\\JMSch\\Documents\\MEGA\\05_Paris\\ESCP Business School\\MSc Big Data & Business Analytics\\Research Methods II\\Sentiment analysis\\eolienne-2k-tweets.csv', dtype=str)\n",
    "\n",
    "\n",
    "train_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP I: TRANSLATION\n",
    "## https://stackabuse.com/text-translation-with-google-translate-api-in-python/\n",
    "## Translating the tweets to English and saving the translations as a new column\n",
    "translation = []\n",
    "\n",
    "NumberOfIterations = train_tweets.count  \n",
    "start = time.time() \n",
    "\n",
    "for i, row in train_tweets.iterrows():\n",
    "    #try:\n",
    "        translator = Translator()\n",
    "        ## print initial tweet value\n",
    "        tweet = str(row['Text'])\n",
    "        print(tweet)\n",
    "        tweet_en = translator.translate(tweet)\n",
    "        ## print the english translation of the tweet\n",
    "        print (tweet_en.text)\n",
    "        ## add the translation to a list\n",
    "        translation.append(tweet_en.text)\n",
    "        PercentageFinished = (i/NumberOfIterations)*100\n",
    "        end = time.time() \n",
    "        elapsed = end - start\n",
    "        TimeToGo = elapsed/i*(NumberOfIterations)\n",
    "        print(f\"{TimeToGo} seconds remaining... ({PercentageFinished}% complete)\")\n",
    "    #except:\n",
    "        translation.append('error')\n",
    "        #pass\n",
    "\n",
    "## append list as a column to the dataframe\n",
    "train_tweets['Translations'] = translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP II: TEXT MANIPULATION\n",
    "\n",
    "## Text manipulation; see further comments --> make a decision about removing short words\n",
    "print(train_tweets['Translations'])\n",
    "\n",
    "## Remove unwanted text patterns from the tweets.\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt) \n",
    "    return input_txt\n",
    "\n",
    "##  Removing Twitter Handles (@user)\n",
    "train_tweets['Tidy Text'] = np.vectorize(remove_pattern)(train_tweets['Translations'], \"@[\\w]*\") \n",
    "##  Removing Punctuations, Numbers, and Special Characters\n",
    "train_tweets['Tidy Text'] = train_tweets['Tidy Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "print(train_tweets['Tidy Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['Translations']\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP IIIA: TOKENISATION\n",
    "\n",
    "## Tokenisation of words, and removing stopwords from the translated tweet contect\n",
    "token_transl = []\n",
    "\n",
    "for i, row in train_tweets.iterrows():\n",
    "    if str(row['Tidy Text']) != \"\":\n",
    "        #try:\n",
    "            ## row['Translation'] --> select one column FROM the ROW \n",
    "            sentence = str(row['Tidy Text'])\n",
    "            stop_words = set(stopwords.words('english'))  \n",
    "            word_tokens = word_tokenize(sentence) \n",
    "            filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "            print(filtered_sentence) \n",
    "            token_transl.append(filtered_sentence)\n",
    "            #print(filtered_sentence)      \n",
    "        #except:\n",
    "        #    pass\n",
    "\n",
    "train_tweets['Tokenised Translation'] = token_transl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP IIIB: SPELL CORRECTION\n",
    "\n",
    "## Text manipulation; see further comments --> make a decision about removing short words\n",
    "print(train_tweets['Tokenised Translation'])\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "for i, row in train_tweets.iterrows():\n",
    "    try:\n",
    "        ## print initial tweet value\n",
    "        tweet = str(row['Tokenised Translation'])\n",
    "        print(tweet)\n",
    "# find those words that may be misspelled\n",
    "        misspelled = spell.unknown(tweet)\n",
    "        for word in misspelled:\n",
    "            # Get the one `most likely` answer\n",
    "            print(spell.correction(word))\n",
    "\n",
    "            # Get a list of `likely` options\n",
    "            print(spell.candidates(word))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP IV: SENTIMENT CALCULATION\n",
    "\n",
    "sentiment_score = []\n",
    "\n",
    "for i, row in train_tweets.iterrows():\n",
    "    try:\n",
    "        print(row['Tokenised Translation'])\n",
    "        text = str(row['Tokenised Translation'])\n",
    "        testimonial = TextBlob(text)\n",
    "        testimonial.sentiment\n",
    "        Score = testimonial.sentiment.polarity\n",
    "        print (Score)\n",
    "        sentiment_score.append(Score)\n",
    "    except:\n",
    "        sentiment_score.append('error')\n",
    "        pass\n",
    "\n",
    "train_tweets['Sentiment Score'] = sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.to_csv(r'C:\\Users\\JMSch\\Documents\\MEGA\\05_Paris\\ESCP Business School\\MSc Big Data & Business Analytics\\Research Methods II\\Sentiment Analysis Twitter.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP IV: SENTIMENT CALCULATION (USING DIFFERENT PACKAGE)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for i, row in train_tweets.iterrows():\n",
    "    print(row['Tokenised Translation'])\n",
    "    sentence = str(row['Tokenised Translation'])\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}